{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NEW LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyart.io import uf\n",
    "from gzip import open as gzip_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gzip _io.BufferedReader name='./data/1203/rhi_a/olympex_NPOL1_20151203_144006_rhi_20-40.uf.gz' 0x7f33b8a192b0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyart.core.radar.Radar at 0x7f33f8643b80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uf_filename = \"./data/1105/ppi/olympex_NPOL1_20151105_161254_uf.gz\"\n",
    "# uf_filename = \"./data/1203/rhi_a/olympex_NPOL1_20151203_084005_rhi_00-20.uf.gz\"\n",
    "uf_filename = \"./data/1203/rhi_a/olympex_NPOL1_20151203_144006_rhi_20-40.uf.gz\"\n",
    "with gzip_open(uf_filename, 'rb') as unzipped_file:\n",
    "    print(unzipped_file)\n",
    "    radar = uf.read_uf(\n",
    "        unzipped_file,\n",
    "        file_field_names=True\n",
    "    )\n",
    "radar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0  226  452  678  904 1130 1356 1582 1808 2034 2260 2486 2712 2938\n",
      " 3164 3390 3616 3842 4068 4294]\n",
      "[ 225  451  677  903 1129 1355 1581 1807 2033 2259 2485 2711 2937 3163\n",
      " 3389 3615 3841 4067 4293 4519]\n"
     ]
    }
   ],
   "source": [
    "sweep_start_ray_idx = radar.sweep_start_ray_index['data'][:]\n",
    "sweep_end_ray_idx = radar.sweep_end_ray_index['data'][:]\n",
    "\n",
    "print(sweep_start_ray_idx)\n",
    "print(sweep_end_ray_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cz = (radar.fields['CZ']['data'][:])\n",
    "dr = (radar.fields['DR']['data'][:])\n",
    "rh = (radar.fields['RH']['data'][:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly using the Reader Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data with new API updated\n",
    "# OLYMPEX\n",
    "# (6) \n",
    "#Ground radar NPOL (gpmnpololyx2)  \n",
    "#collection = GroundRadar\n",
    "#campaign = olympex\n",
    "#instrument = NPOL\n",
    "#scan type = RHI\n",
    "\n",
    "from pyart.io import uf\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from numpy import ma\n",
    "from gzip import open as gzip_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UFReader():\n",
    "    \"\"\"Reader that reads all data from a set of UF Radar Files.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path):\n",
    "        self.uf_file = file_path\n",
    "\n",
    "    def _read_radar(self):\n",
    "        \"\"\"Read the radar file and return some important values.\n",
    "\n",
    "        :return: The radar data including some important values.\n",
    "        :rtype: Tuple\n",
    "        \"\"\"\n",
    "        uf_filename = self.uf_file\n",
    "        if uf_filename.endswith('.gz'):\n",
    "            with gzip_open(uf_filename, 'rb') as unzipped_file:\n",
    "                radar = uf.read_uf(\n",
    "                    unzipped_file,\n",
    "                    file_field_names=True\n",
    "                )\n",
    "        else:\n",
    "            radar = uf.read_uf(\n",
    "                uf_filename,\n",
    "                file_field_names=True\n",
    "            )\n",
    "        sweep_start_ray_idx = radar.sweep_start_ray_index['data'][:]\n",
    "        sweep_end_ray_idx = radar.sweep_end_ray_index['data'][:]\n",
    "\n",
    "        cz = (radar.fields['CZ']['data'][:])\n",
    "        dr = (radar.fields['DR']['data'][:])\n",
    "        rh = (radar.fields['RH']['data'][:])\n",
    "        \n",
    "        fh = (radar.fields['FH']['data'][:])\n",
    "        dm = (radar.fields['DM']['data'][:])\n",
    "\n",
    "\n",
    "        # Lucy Wang added the following command lines on July 23, 2018\n",
    "        gate_latitude = radar.gate_latitude['data'][:]\n",
    "        gate_longitude = radar.gate_longitude['data'][:]\n",
    "        gate_altitude = radar.gate_altitude['data'][:]\n",
    "        # -------------------------------------------------------------\n",
    "\n",
    "        # Get time in %Y-%m-%d%H:%M:%SZ format\n",
    "        full_time = radar.time['units']\\\n",
    "            .replace('since', '')\\\n",
    "            .replace('seconds', '')\n",
    "        full_time = full_time.replace(' ', '')\n",
    "        full_time = full_time.replace('T', '')\n",
    "        full_time = datetime.strptime(full_time, '%Y-%m-%d%H:%M:%SZ')\n",
    "\n",
    "        return (\n",
    "            sweep_start_ray_idx,\n",
    "            sweep_end_ray_idx,\n",
    "            cz,\n",
    "            dr,\n",
    "            rh,\n",
    "            fh,\n",
    "            dm,\n",
    "            gate_latitude,\n",
    "            gate_longitude,\n",
    "            gate_altitude,\n",
    "            full_time,\n",
    "            radar,\n",
    "        )\n",
    "\n",
    "    def read_data(self):\n",
    "        \"\"\"Generator function that generates each datum from the radar file\n",
    "        sequentially.\n",
    "        \"\"\"\n",
    "        index = 0\n",
    "        (\n",
    "            sweep_start_ray_idx,\n",
    "            sweep_end_ray_idx,\n",
    "            CZ,\n",
    "            DR,\n",
    "            RH,\n",
    "            FH,\n",
    "            DM,\n",
    "            gate_latitude,\n",
    "            gate_longitude,\n",
    "            gate_altitude,\n",
    "            full_time,\n",
    "            radar\n",
    "        ) = self._read_radar()\n",
    "\n",
    "        # sweep by sweep; 20 sweeps per rhi_a file (over ocean)\n",
    "        for ii in range(0, radar.nsweeps):\n",
    "            # Calculate the start and end time for this sweep\n",
    "            idx0 = sweep_start_ray_idx[ii]\n",
    "            idx1 = sweep_end_ray_idx[ii]\n",
    "\n",
    "            # ray by ray; 226 rays per sweep\n",
    "            for ray in range(idx0, idx1 + 1):\n",
    "                tmp_cz = CZ[ray, :]\n",
    "                tmp_dr = DR[ray, :]\n",
    "                tmp_rh = RH[ray, :]\n",
    "                tmp_fh = FH[ray, :]\n",
    "                tmp_dm = DM[ray, :]\n",
    "                tmp_time_ray = (full_time + timedelta(\n",
    "                    seconds=float(radar.time['data'][ray])\n",
    "                )).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "                tmp_gate_lat = gate_latitude[ray, :]\n",
    "                tmp_gate_lon = gate_longitude[ray, :]\n",
    "                tmp_gate_alt = gate_altitude[ray, :]\n",
    "\n",
    "                # check CZ values gate by gate; 1081 gates per ray\n",
    "                for gate in range(0, len(tmp_cz)):\n",
    "                    if tmp_cz[gate] is not ma.masked:\n",
    "                        row_dict = {\n",
    "                            'timestamp': tmp_time_ray,\n",
    "                            'lat': round(float(tmp_gate_lat[gate]), 4),\n",
    "                            'lon': round(float(tmp_gate_lon[gate]), 4),\n",
    "                            'height': float(tmp_gate_alt[gate]),\n",
    "                            'CZ': float(tmp_cz[gate]),\n",
    "                            'DR': float(tmp_dr[gate]),\n",
    "                            'RH': float(tmp_rh[gate]),\n",
    "                            'FH': float(tmp_fh[gate]),\n",
    "                            'DM': float(tmp_dm[gate]),\n",
    "                        }\n",
    "                        yield row_dict\n",
    "                        index += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUST EXTRACTING DATA TO NEEDED NP FORM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken =>>> 12.56305\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "uf_filename = \"./data/1203/rhi_a/olympex_NPOL1_20151203_000005_rhi_00-20.uf.gz\"\n",
    "\n",
    "# uf_filename = \"./data/1203/rhi_a/olympex_NPOL1_20151203_144006_rhi_20-40.uf.gz\"\n",
    "ufr = UFReader(uf_filename)\n",
    "uf_datas = ufr.read_data() # it will return a generator.\n",
    "\n",
    "atb = np.array([], dtype=np.int64)            \n",
    "lon = np.array([], dtype=np.int64)\n",
    "lat = np.array([], dtype=np.int64)\n",
    "alt = np.array([], dtype=np.int64)\n",
    "time = np.array([], dtype=np.int64)\n",
    "\n",
    "t1 = datetime.now()\n",
    "\n",
    "for uf_data in list(uf_datas):\n",
    "    pass\n",
    "    # atb = np.append(atb, uf_data['CZ'])\n",
    "    # lon = np.append(lon, uf_data['lon'])\n",
    "    # lat = np.append(lat, uf_data['lat'])\n",
    "    # alt = np.append(alt, uf_data['height'])\n",
    "    # time = np.append(time, np.datetime64(uf_data['timestamp']).astype('timedelta64[s]').astype(np.int64))\n",
    "\n",
    "t2 = datetime.now()\n",
    "\n",
    "print(\"time taken =>>>\", (t2-t1).total_seconds())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INGEST (CHUNKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object UFReader.read_data at 0x7f91b592fa50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3359/4223549215.py:19: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  time = np.append(time, np.datetime64(uf_data['timestamp']))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rd = UFReader('./data/1203/rhi_a/olympex_NPOL1_20151203_000005_rhi_00-20.uf.gz')\n",
    "uf_datas = rd.read_data()\n",
    "\n",
    "print(uf_datas)\n",
    "\n",
    "atb = np.array([], dtype=np.int64)            \n",
    "lon = np.array([], dtype=np.int64)\n",
    "lat = np.array([], dtype=np.int64)\n",
    "alt = np.array([], dtype=np.int64)\n",
    "time = np.array([], dtype=np.datetime64)\n",
    "\n",
    "for uf_data in uf_datas:\n",
    "    atb = np.append(atb, uf_data['CZ'])\n",
    "    lon = np.append(atb, uf_data['lon'])\n",
    "    lat = np.append(atb, uf_data['lat'])\n",
    "    alt = np.append(atb, uf_data['height'])\n",
    "    time = np.append(time, np.datetime64(uf_data['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97150,) :  [25.93 35.02 27.47 ...  5.16 10.2   5.23]\n",
      "(97151,) :  [  25.93     35.02     27.47   ...   10.2       5.23   -124.2269]\n",
      "(97151,) :  [25.93   35.02   27.47   ... 10.2     5.23   47.2765]\n",
      "(97151,) :  [  25.93   35.02   27.47 ...   10.2     5.23 1350.  ]\n",
      "(97150,) :  ['2015-12-03T00:00:07' '2015-12-03T00:00:07' '2015-12-03T00:00:07' ...\n",
      " '2015-12-03T00:05:49' '2015-12-03T00:05:49' '2015-12-03T00:05:49']\n"
     ]
    }
   ],
   "source": [
    "print(atb.shape, \": \",atb)\n",
    "print(lon.shape, \": \",lon)\n",
    "print(lat.shape, \": \",lat)\n",
    "print(alt.shape, \": \",alt)\n",
    "print(time.shape, \": \",time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INGEST ALL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-REQUISITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from threading import Thread, Lock\n",
    "\n",
    "to_rad = np.pi / 180.0\n",
    "to_deg = 180.0 / np.pi\n",
    "\n",
    "steps = [32, 16, 8, 4, 2, 1]\n",
    "\n",
    "class PointCloud:\n",
    "    def __init__(self, key, lon, lat, alt, value, time, epoch):\n",
    "        self.key = key\n",
    "        self.lon = lon\n",
    "        self.lat = lat\n",
    "        self.alt = alt\n",
    "        self.time = time\n",
    "        self.value = value\n",
    "        self.epoch = epoch\n",
    "        self.tasks = []\n",
    "        self.threads = []\n",
    "        for i in range(10):\n",
    "            self.threads.append(Thread(target=self.worker_function))\n",
    "        self.tileset_lock = Lock()\n",
    "        self.tileset_json = {\n",
    "        \t\"asset\": {\n",
    "        \t\t\"version\": \"1.0\",\n",
    "        \t\t\"type\": \"Airborne Radar\"\n",
    "        \t},\n",
    "        \t\"root\": {\n",
    "        \t\t\"geometricError\": 1000000,\n",
    "        \t\t\"refine\" : \"REPLACE\",\n",
    "        \t\t\"boundingVolume\": {\n",
    "                    \"region\": [\n",
    "                        float(np.min(lon)) * to_rad,\n",
    "                        float(np.min(lat)) * to_rad,\n",
    "                        float(np.max(lon)) * to_rad,\n",
    "                        float(np.max(lat)) * to_rad,\n",
    "                        float(np.min(alt)) * to_rad,\n",
    "                        float(np.max(alt)) * to_rad\n",
    "                    ]\n",
    "                },\n",
    "                \"children\": []\n",
    "        \t},\n",
    "            \"properties\": {\n",
    "                \"epoch\": \"{}Z\".format(datetime.utcfromtimestamp(epoch).isoformat()),\n",
    "                \"refined\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def worker_function(self):\n",
    "        while len(self.tasks) > 0:\n",
    "                tile, start, end = self.tasks.pop()\n",
    "                print(tile, start, end)\n",
    "                self.generate(tile, start, end)\n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        for t in self.threads:\n",
    "            t.start()\n",
    "\n",
    "\n",
    "    def join(self):\n",
    "        for t in self.threads:\n",
    "            t.join()\n",
    "        with open('{}/tileset.json'.format(self.key), mode='w+') as outfile:\n",
    "            json.dump(self.tileset_json, outfile)\n",
    "\n",
    "\n",
    "    def schedule_task(self, tile, start, end):\n",
    "        self.tasks.append((tile, start, end))\n",
    "\n",
    "\n",
    "    def generate(self, tile, start, end):\n",
    "        print(tile, start, end)\n",
    "        parent_tile = self.tileset_json[\"root\"]\n",
    "        cartesian, offset, scale, cartographic, region = self.cartographic_to_cartesian(start, end)\n",
    "\n",
    "        value = self.value[start:end]\n",
    "        time = self.time[start:end]\n",
    "\n",
    "        epoch = int(np.min(time) + self.epoch - 300)\n",
    "        epoch = \"{}Z\".format(datetime.utcfromtimestamp(epoch).isoformat())\n",
    "        end = int(np.max(time) + self.epoch + 300)\n",
    "        end = \"{}Z\".format(datetime.utcfromtimestamp(end).isoformat())\n",
    "\n",
    "        header_length = 28\n",
    "        magic = np.string_(\"pnts\")\n",
    "        version = 1\n",
    "\n",
    "        for step in steps:\n",
    "            self.tileset_lock.acquire()\n",
    "            try:\n",
    "                filename = \"{}_{}.pnts\".format(tile, step)\n",
    "                child_tile = {\n",
    "                    \"availability\": \"{}/{}\".format(epoch, end),\n",
    "                    \"geometricError\": step * 500,\n",
    "                    \"boundingVolume\": {\n",
    "                        \"region\": region\n",
    "                    },\n",
    "                    \"content\": {\n",
    "                        \"uri\": filename\n",
    "                    },\n",
    "                    \"refine\": \"REPLACE\"\n",
    "                }\n",
    "                if step == 1:\n",
    "                    self.tileset_json[\"properties\"][\"refined\"].append(filename)\n",
    "                else:\n",
    "                    child_tile[\"children\"] = []\n",
    "                parent_tile[\"children\"].append(child_tile)\n",
    "                parent_tile = child_tile\n",
    "            finally:\n",
    "                self.tileset_lock.release()\n",
    "\n",
    "            tile_length = 0\n",
    "            feature_table_binary_byte_length = 0\n",
    "            batch_table_binary_byte_length = 0\n",
    "            length = value[::step].size\n",
    "\n",
    "            feature_table_json = {\n",
    "                \"POINTS_LENGTH\": length,\n",
    "                \"BATCH_LENGTH\": length,\n",
    "                \"BATCH_ID\": {\n",
    "                    \"byteOffset\": 0,\n",
    "                    \"componentType\": \"UNSIGNED_INT\"\n",
    "                },\n",
    "                \"POSITION_QUANTIZED\": {\n",
    "                    \"byteOffset\": length * 4\n",
    "                },\n",
    "                \"QUANTIZED_VOLUME_OFFSET\": offset,\n",
    "                \"QUANTIZED_VOLUME_SCALE\": scale\n",
    "            }\n",
    "\n",
    "            batch_table_json = {\n",
    "                \"value\": {\n",
    "                    \"byteOffset\": 0,\n",
    "                    \"componentType\": \"FLOAT\",\n",
    "                    \"type\": \"SCALAR\"\n",
    "                },\n",
    "                \"time\": {\n",
    "                    \"byteOffset\": length * 4,\n",
    "                    \"componentType\": \"FLOAT\",\n",
    "                    \"type\": \"SCALAR\"\n",
    "                },\n",
    "                \"location\": {\n",
    "                    \"byteOffset\": length * 8,\n",
    "                    \"componentType\": \"SHORT\",\n",
    "                    \"type\": \"VEC3\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            tile_length += header_length\n",
    "\n",
    "            feature_table_json_min = json.dumps(feature_table_json, separators=(\",\", \":\")) + \"       \"\n",
    "            feature_table_trim = (tile_length + len(feature_table_json_min)) % 8\n",
    "            if feature_table_trim != 0:\n",
    "                feature_table_json_min = feature_table_json_min[:-feature_table_trim]\n",
    "\n",
    "            tile_length += len(feature_table_json_min)\n",
    "\n",
    "            feature_table_binary_byte_length = length * 4 + length * 3 * 2\n",
    "            tile_length += feature_table_binary_byte_length\n",
    "            feature_table_padding = tile_length % 8\n",
    "            if feature_table_padding != 0:\n",
    "                feature_table_padding = 8 - feature_table_padding\n",
    "            tile_length += feature_table_padding\n",
    "\n",
    "            batch_table_json_min = json.dumps(batch_table_json, separators=(\",\", \":\")) + \"       \"\n",
    "            batch_table_trim = (tile_length + len(batch_table_json_min)) % 8\n",
    "            if batch_table_trim != 0:\n",
    "                batch_table_json_min = batch_table_json_min[:-batch_table_trim]\n",
    "\n",
    "            tile_length += len(batch_table_json_min)\n",
    "\n",
    "            batch_table_binary_byte_length = length * 4 * 2 + length * 2 * 3\n",
    "            tile_length += batch_table_binary_byte_length\n",
    "            batch_table_padding = tile_length % 8\n",
    "            if batch_table_padding != 0:\n",
    "                batch_table_padding = 8 - batch_table_padding\n",
    "            tile_length += batch_table_padding\n",
    "\n",
    "            with open('{}/{}'.format(self.key, filename), mode='wb+') as outfile:\n",
    "                outfile.write(np.string_(magic).tobytes())\n",
    "                outfile.write(np.uint32(version).tobytes())\n",
    "                outfile.write(np.uint32(tile_length).tobytes())\n",
    "                outfile.write(np.uint32(len(feature_table_json_min)).tobytes())\n",
    "                outfile.write(np.uint32(feature_table_binary_byte_length + feature_table_padding).tobytes())\n",
    "                outfile.write(np.uint32(len(batch_table_json_min)).tobytes())\n",
    "                outfile.write(np.uint32(batch_table_binary_byte_length + batch_table_padding).tobytes())\n",
    "                outfile.write(np.string_(feature_table_json_min).tobytes())\n",
    "                outfile.write(np.arange(length, dtype=np.uint32).tobytes())\n",
    "                outfile.write(cartesian[::step, :].tobytes())\n",
    "                for _ in range(feature_table_padding):\n",
    "                    outfile.write(np.string_(\" \").tobytes())\n",
    "                outfile.write(np.string_(batch_table_json_min).tobytes())\n",
    "                outfile.write(value[::step].astype(np.float32).tobytes())\n",
    "                outfile.write(time[::step].astype(np.float32).tobytes())\n",
    "                outfile.write(cartographic[::step, :].tobytes())\n",
    "                for _ in range(batch_table_padding):\n",
    "                    outfile.write(np.string_(\" \").tobytes())\n",
    "                outfile.seek(0)\n",
    "\n",
    "\n",
    "    def cartographic_to_cartesian(self, start, end):\n",
    "        lon = self.lon[start:end]\n",
    "        lat = self.lat[start:end]\n",
    "        alt = self.alt[start:end]\n",
    "        size = lon.size\n",
    "\n",
    "        cartographic = np.zeros(shape=(size, 3), dtype=np.int16)\n",
    "        cartographic[:, 0] = (lon * 32767 / 180).astype(np.int16)\n",
    "        cartographic[:, 1] = (lat * 32767 / 180).astype(np.int16)\n",
    "        cartographic[:, 2] = (alt / 10).astype(np.int16)\n",
    "\n",
    "        lon = lon * to_rad\n",
    "        lat = lat * to_rad\n",
    "\n",
    "        radiiSquared = np.array([40680631590769, 40680631590769, 40408299984661.445], dtype=np.float64)\n",
    "\n",
    "        N1 = np.multiply(np.cos(lat), np.cos(lon))\n",
    "        N2 = np.multiply(np.cos(lat), np.sin(lon))\n",
    "        N3 = np.sin(lat)\n",
    "\n",
    "        magnitude = np.sqrt(np.square(N1) + np.square(N2) + np.square(N3))\n",
    "\n",
    "        N1 = N1 / magnitude\n",
    "        N2 = N2 / magnitude\n",
    "        N3 = N3 / magnitude\n",
    "\n",
    "        K1 = radiiSquared[0] * N1\n",
    "        K2 = radiiSquared[1] * N2\n",
    "        K3 = radiiSquared[2] * N3\n",
    "\n",
    "        gamma = np.sqrt(np.multiply(N1, K1) + np.multiply(N2, K2) + np.multiply(N3, K3))\n",
    "\n",
    "        K1 = K1 / gamma\n",
    "        K2 = K2 / gamma\n",
    "        K3 = K3 / gamma\n",
    "\n",
    "        N1 = np.multiply(N1, alt)\n",
    "        N2 = np.multiply(N2, alt)\n",
    "        N3 = np.multiply(N3, alt)\n",
    "\n",
    "        # x = np.multiply((N1 + K1), np.random.normal(1, .00005, N1.size))\n",
    "        # y = np.multiply((N2 + K2), np.random.normal(1, .00005, N1.size))\n",
    "        # z = np.multiply((N3 + K3), np.random.normal(1, .00005, N1.size))\n",
    "\n",
    "        x = N1 + K1\n",
    "        y = N2 + K2\n",
    "        z = N3 + K3\n",
    "\n",
    "        offset = [float(np.min(x)), float(np.min(y)), float(np.min(z))]\n",
    "\n",
    "        x = x - offset[0]\n",
    "        y = y - offset[1]\n",
    "        z = z - offset[2]\n",
    "\n",
    "        scale = [float(abs(np.max(x))), float(abs(np.max(y))), float(abs(np.max(z)))]\n",
    "\n",
    "        cartesian = np.zeros(shape=(size, 3), dtype=np.uint16)\n",
    "        cartesian[:, 0] = (x / scale[0] * 65535.0).astype(np.uint16)\n",
    "        cartesian[:, 1] = (y / scale[1] * 65535.0).astype(np.uint16)\n",
    "        cartesian[:, 2] = (z / scale[2] * 65535.0).astype(np.uint16)\n",
    "\n",
    "        region = [\n",
    "            float(np.min(lon)),\n",
    "            float(np.min(lat)),\n",
    "            float(np.max(lon)),\n",
    "            float(np.max(lat)),\n",
    "            float(np.min(alt)),\n",
    "            float(np.max(alt))\n",
    "        ]\n",
    "\n",
    "        return cartesian, offset, scale, cartographic, region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import zarr\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "to_rad = np.pi / 180.0\n",
    "to_deg = 180.0 / np.pi\n",
    "\n",
    "def generate_point_cloud(variable, epoch, end, zarr_location, point_cloud_folder):\n",
    "    \"\"\"Generates json pointcloud from a given zarr file input\n",
    "\n",
    "    Args:\n",
    "        variable (_type_): _description_\n",
    "        epoch (_type_): _description_\n",
    "        end (_type_): _description_\n",
    "        zarr_location (string): source zarr file.\n",
    "        point_cloud_folder (string): destination folder for 3d tile json file.\n",
    "    \"\"\"\n",
    "\n",
    "    #out_key = f\"{os.getenv('CRS_OUTPUT_FLIGHT_PATH')}/{shortname}\"\n",
    "    #pc_out_key = f\"{output_path}/point_cloud\"\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        os.mkdir(out_key)\n",
    "    except:\n",
    "        pass\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        os.mkdir(point_cloud_folder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # LOAD THE DATA.\n",
    "    store = zarr.DirectoryStore(zarr_location)\n",
    "    root = zarr.group(store=store)\n",
    "\n",
    "    chunk_id = root[\"chunk_id\"][:]\n",
    "    num_chunks = chunk_id.shape[0]\n",
    "    id = np.argmax(chunk_id[:, 1] > epoch) - 1\n",
    "    start_id = chunk_id[0 if id < 0 else id, 0]\n",
    "    id = num_chunks - np.argmax(chunk_id[::-1, 1] < end)\n",
    "    end_id = chunk_id[id, 0] if id < num_chunks else root[\"time\"].size - 1\n",
    "\n",
    "    root_epoch = root.attrs[\"epoch\"]\n",
    "    location = root[\"location\"][start_id:end_id]\n",
    "    lon = location[:, 0]\n",
    "    lat = location[:, 1]\n",
    "    alt = location[:, 2]\n",
    "    value = root[\"value\"][variable][start_id:end_id]\n",
    "    time = root[\"time\"][start_id:end_id]\n",
    "\n",
    "    # filter data using mask\n",
    "    epoch = epoch - root_epoch # date-time\n",
    "    end = end - root_epoch\n",
    "    mask = np.logical_and(time >= epoch, time <= end)\n",
    "    lon = lon[mask]\n",
    "    lat = lat[mask]\n",
    "    alt = alt[mask]\n",
    "    value = value[mask]\n",
    "    time = time[mask]\n",
    "\n",
    "    # Generate Pointcloud Tileset\n",
    "    point_cloud = PointCloud(point_cloud_folder, lon, lat, alt, value, time, root_epoch)\n",
    "\n",
    "    for tile in range(int(np.ceil(time.size / 530000))):\n",
    "        start_id = tile * 530000\n",
    "        end_id = np.min([start_id + 530000, time.size])\n",
    "        point_cloud.schedule_task(tile, start_id, end_id)\n",
    "\n",
    "    point_cloud.start()\n",
    "    point_cloud.join()\n",
    "\n",
    "tileset_json = {\n",
    "\t\"asset\": {\n",
    "\t\t\"version\": \"1.0\",\n",
    "\t\t\"type\": \"Airborne Radar\"\n",
    "\t},\n",
    "\t\"root\": {\n",
    "\t\t\"geometricError\": 1000000,\n",
    "\t\t\"refine\" : \"REPLACE\",\n",
    "\t\t\"boundingVolume\": {\n",
    "            \"region\": []\n",
    "        },\n",
    "        \"children\": []\n",
    "\t},\n",
    "    \"properties\": {\n",
    "        \"epoch\": \"\",\n",
    "        \"refined\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main(sys.argv[1], sys.argv[2], int(sys.argv[3]), int(sys.argv[4]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zarr\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import shutil\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "import s3fs\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from boto3 import client as boto_client\n",
    "import tarfile\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META needed for ingest\n",
    "campaign = 'Olympex'\n",
    "collection = \"AirborneRadar\"\n",
    "dataset = \"gpmValidationOlympexcrs\"\n",
    "variables = [\"ref\"]\n",
    "renderers = [\"point_cloud\"]\n",
    "chunk = 262144\n",
    "to_rad = np.pi / 180\n",
    "to_deg = 180 / np.pi\n",
    "\n",
    "def ingest(folder, filePath):\n",
    "    \"\"\"\n",
    "    Converts Level 1B crs data from s3 to zarr file and then stores it in the provided folder\n",
    "    Args:\n",
    "        folder (string): name to hold the raw files.\n",
    "        file (string): the s3 url to the raw file. WHAT FORMAT IS IT IN in hdf5 format\n",
    "    \"\"\"\n",
    "    store = zarr.DirectoryStore(folder)\n",
    "    root = zarr.group(store=store)\n",
    "    \n",
    "    # Create empty rows for modified data    \n",
    "    z_chunk_id = root.create_dataset('chunk_id', shape=(0, 2), chunks=None, dtype=np.int64)\n",
    "    z_location = root.create_dataset('location', shape=(0, 3), chunks=(chunk, None), dtype=np.float32)\n",
    "    z_time = root.create_dataset('time', shape=(0), chunks=(chunk), dtype=np.int32)\n",
    "    z_vars = root.create_group('value')\n",
    "    z_ref = z_vars.create_dataset('atb', shape=(0), chunks=(chunk), dtype=np.float32)\n",
    "    n_time = np.array([], dtype=np.int64)\n",
    "\n",
    "    # date = file.split(\"_\")[5].split(\".\")[0]\n",
    "    # base_time = np.datetime64('{}-{}-{}'.format(date[:4], date[4:6], date[6:]))\n",
    "\n",
    "    print(\"Accessing file to convert to zarr \")\n",
    "    \n",
    "    # !!! input uf file path is inside UFREADER\n",
    "    ufr = UFReader(filePath)\n",
    "    uf_datas = ufr.read_data() # it will return a generator.\n",
    "\n",
    "    # using the generator, populate all the lon, lat, alt and atb values\n",
    "\n",
    "    atb = np.array([], dtype=np.int64)            \n",
    "    lon = np.array([], dtype=np.int64)\n",
    "    lat = np.array([], dtype=np.int64)\n",
    "    alt = np.array([], dtype=np.int64)\n",
    "    time = np.array([], dtype=np.int64)\n",
    "\n",
    "    for uf_data in uf_datas:\n",
    "        atb = np.append(atb, uf_data['CZ'])\n",
    "        lon = np.append(lon, uf_data['lon'])\n",
    "        lat = np.append(lat, uf_data['lat'])\n",
    "        alt = np.append(alt, uf_data['height'])\n",
    "        time = np.append(time, np.datetime64(uf_data['timestamp']).astype('timedelta64[s]').astype(np.int64))\n",
    "    \n",
    "\n",
    "    sort_idx = np.argsort(time)\n",
    "\n",
    "    lon = lon[sort_idx]\n",
    "    lat = lat[sort_idx]\n",
    "    alt = alt[sort_idx]\n",
    "    atb = atb[sort_idx]\n",
    "    time = time[sort_idx]\n",
    "\n",
    "    # Now populate (append) the empty rows with modified data.\n",
    "    z_location.append(np.stack([lon, lat, alt], axis=-1))\n",
    "    z_ref.append(atb)\n",
    "\n",
    "    n_time = np.append(n_time, time)\n",
    "\n",
    "    idx = np.arange(0, n_time.size, chunk)\n",
    "    chunks = np.zeros(shape=(idx.size, 2), dtype=np.int64)\n",
    "    chunks[:, 0] = idx\n",
    "    chunks[:, 1] = n_time[idx]\n",
    "    z_chunk_id.append(chunks)\n",
    "\n",
    "    epoch = np.min(n_time)\n",
    "    n_time = (n_time - epoch).astype(np.int32)\n",
    "    z_time.append(n_time)\n",
    "\n",
    "    # save it.\n",
    "    root.attrs.put({\n",
    "        \"campaign\": campaign,\n",
    "        \"collection\": collection,\n",
    "        \"dataset\": dataset,\n",
    "        \"variables\": variables,\n",
    "        \"renderers\": renderers,\n",
    "        \"epoch\": int(epoch)\n",
    "    })\n",
    "\n",
    "def downloadFromS3(bucket_name, s3_key, dest_dir):\n",
    "    s3 = boto_client('s3')\n",
    "    filename = s3_key.split('/')[3]\n",
    "    dest_dir = '/tmp/npol_olympex/raw/'\n",
    "    dest = dest_dir + filename\n",
    "    if os.path.exists(dest_dir): shutil.rmtree(f\"{dest_dir}\")\n",
    "    Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Downloading file\",s3_key,\"from bucket\",bucket_name, \" into dir:\", dest_dir)\n",
    "    s3.download_file(\n",
    "        Bucket = bucket_name,\n",
    "        Key = s3_key,\n",
    "        Filename = dest\n",
    "    )\n",
    "    return dest\n",
    "\n",
    "\n",
    "def untarr(raw_file_dir, raw_file_path, filename):\n",
    "    unzipped_file_path = raw_file_dir + filename.split(\".\")[0] # removing the .tar.gz # this is important\n",
    "    if raw_file_path.endswith(\"tar.gz\"):\n",
    "        with tarfile.open(raw_file_path, \"r:gz\") as t:\n",
    "            t.extractall(unzipped_file_path)\n",
    "    elif raw_file_path.endswith(\"tar\"):\n",
    "        with tarfile.open(raw_file_path, \"r:\") as t:\n",
    "            t.extractall(unzipped_file_path)\n",
    "    return unzipped_file_path\n",
    "# ------------------START--------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0. converting for ./data/1203/rhi_a/olympex_NPOL1_20151203_144006_rhi_20-40.uf.gz\n",
      "Accessing file to convert to zarr \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3359/226716490.py:51: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  time = np.append(time, np.datetime64(uf_data['timestamp']).astype('timedelta64[s]').astype(np.int64))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m     instrument_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnpol\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m     data_pre_process(bucket_name, field_campaign, input_data_dir, output_data_dir, instrument_name)\n\u001b[0;32m---> 64\u001b[0m npol()\n",
      "Cell \u001b[0;32mIn[36], line 61\u001b[0m, in \u001b[0;36mnpol\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m output_data_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minstrument-processed-data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m instrument_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnpol\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 61\u001b[0m data_pre_process(bucket_name, field_campaign, input_data_dir, output_data_dir, instrument_name)\n",
      "Cell \u001b[0;32mIn[36], line 42\u001b[0m, in \u001b[0;36mdata_pre_process\u001b[0;34m(bucket_name, field_campaign, input_data_dir, output_data_dir, instrument_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m# LOAD FROM SOURCE WITH NECESSARY PRE PROCESSING. CONVERT LEVEL 1B RAW FILES INTO ZARR FILE.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# src_s3_path = f\"s3://{bucket_name}/{s3_raw_file_key}\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m src_s3_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mabc\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m ingest(folder, minute_data_path)\n\u001b[1;32m     43\u001b[0m \u001b[39m# # CONVERT ZARR FILE INTO 3D TILESET JSON.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m generate_point_cloud(\u001b[39m\"\u001b[39m\u001b[39matb\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39m0\u001b[39m,  \u001b[39m1000000000000\u001b[39m, folder, point_cloud_folder)\n",
      "Cell \u001b[0;32mIn[31], line 51\u001b[0m, in \u001b[0;36mingest\u001b[0;34m(folder, filePath)\u001b[0m\n\u001b[1;32m     49\u001b[0m     lat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(lat, uf_data[\u001b[39m'\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     50\u001b[0m     alt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(alt, uf_data[\u001b[39m'\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 51\u001b[0m     time \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mappend(time, np\u001b[39m.\u001b[39;49mdatetime64(uf_data[\u001b[39m'\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mtimedelta64[s]\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mint64))\n\u001b[1;32m     54\u001b[0m sort_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(time)\n\u001b[1;32m     56\u001b[0m lon \u001b[39m=\u001b[39m lon[sort_idx]\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/itsc-fcx/lib/python3.9/site-packages/numpy/lib/function_base.py:5499\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5497\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[1;32m   5498\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 5499\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def data_pre_process(bucket_name, field_campaign, input_data_dir, output_data_dir, instrument_name):\n",
    "    # for s3_raw_file_key in keys:\n",
    "    # download each input file.\n",
    "    # unzip it\n",
    "    # go inside rhi_a dir,\n",
    "    # list all the files.\n",
    "    # for each file, run ingest.\n",
    "    # generate point clouds.\n",
    "    # upload all of the pointcloud files.\n",
    "\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3bucket = s3_resource.Bucket(bucket_name)    \n",
    "    keys = []\n",
    "    for obj in s3bucket.objects.filter(\n",
    "            Prefix=f\"{field_campaign}/{input_data_dir}/{instrument_name}/olympex_npol\"):\n",
    "        keys.append(obj.key)\n",
    "\n",
    "    raw_file_dir = '/tmp/npol_olympex/raw/' # local dir where raw file resides.\n",
    "\n",
    "    for s3_key in keys:\n",
    "        filename = s3_key.split('/')[3]\n",
    "        # raw_file_path = downloadFromS3(bucket_name, s3_key, raw_file_dir) # inc file name\n",
    "        # the raw file is for a single day. When unzipped, it will contain several data collected every 20 mins\n",
    "        # unzipped_file_path = untarr(raw_file_dir, raw_file_path, filename)        \n",
    "        # minutely_datas = glob.glob(f\"{unzipped_file_path}/*/rhi_a/*.uf.gz\")\n",
    "        # minutely_datas = [\"./data/1203/rhi_a/olympex_NPOL1_20151203_000005_rhi_00-20.uf.gz\"] #6mb\n",
    "        minutely_datas = [\"./data/1203/rhi_a/olympex_NPOL1_20151203_144006_rhi_20-40.uf.gz\"] #21mb\n",
    "        for index, minute_data_path in enumerate(minutely_datas):\n",
    "            print(f\"\\n{index}. converting for {minute_data_path}\")\n",
    "            # convert and save.\n",
    "            # # SOURCE DIR.\n",
    "            sdate = minute_data_path.split(\"/\")[-1].split(\"_\")[2]\n",
    "            # CREATE A LOCAL DIR TO HOLD RAW DATA AND CONVERTED DATA\n",
    "            folder = f\"/tmp/npol_olympex/zarr/{sdate}/freq-{index}\" # intermediate folder for zarr file (date + time), time rep by index.\n",
    "            point_cloud_folder = f\"{folder}/point_cloud\" # intermediate folder for 3d tiles, point cloud\n",
    "            if os.path.exists(folder): shutil.rmtree(f\"{folder}\")\n",
    "            # os.mkdir(folder)\n",
    "            Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "            # LOAD FROM SOURCE WITH NECESSARY PRE PROCESSING. CONVERT LEVEL 1B RAW FILES INTO ZARR FILE.\n",
    "            # src_s3_path = f\"s3://{bucket_name}/{s3_raw_file_key}\"\n",
    "            src_s3_path = \"abc\"\n",
    "            ingest(folder, minute_data_path)\n",
    "            # # CONVERT ZARR FILE INTO 3D TILESET JSON.\n",
    "            generate_point_cloud(\"atb\",  0,  1000000000000, folder, point_cloud_folder)\n",
    "            # # UPLOAD CONVERTED FILES.\n",
    "            files = os.listdir(point_cloud_folder)\n",
    "            print(files)\n",
    "            # for file in files:\n",
    "            #     fname = os.path.join(point_cloud_folder, file) # SOURCE\n",
    "            #     s3name = f\"{field_campaign}/{output_data_dir}/npol/{sdate}/freq-{index}/{file}\" # DESTINATION\n",
    "            #     print(f\"uploaded to {s3name}.\")\n",
    "            #     upload_to_s3(fname, bucket_name, s3_name=s3name)\n",
    "\n",
    "def npol():\n",
    "    # bucket_name = os.getenv('RAW_DATA_BUCKET')\n",
    "    bucket_name=\"ghrc-fcx-field-campaigns-szg\"\n",
    "    field_campaign = \"Olympex\"\n",
    "    input_data_dir = \"instrument-raw-data\"\n",
    "    output_data_dir = \"instrument-processed-data\"\n",
    "    instrument_name = \"npol\"\n",
    "    data_pre_process(bucket_name, field_campaign, input_data_dir, output_data_dir, instrument_name)\n",
    "\n",
    "\n",
    "npol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun(x):\n",
    "    n = 0\n",
    "    while n < x:\n",
    "        yield n\n",
    "        n += 1\n",
    "z = fun(10)\n",
    "next(z)\n",
    "next(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsc-fcx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
